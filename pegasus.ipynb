{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install the necessary libraries here\n\n!pip install transformers -q\n!pip install wandb\n\n!pip install rouge-score\n!pip install shap\n!pip install sentencepiece\nimport torch.nn as nn\n\n# Code for TPU packages install\n# !curl -q https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport os","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# os.chdir(\"D:/Datasets/453_NLP_Final_Project\")\n#os.environ['TRANSFORMERS_CACHE'] = 'D:/huggingface/transformers'\n#os.environ['HF_DATASETS_CACHE'] = 'D:/huggingface/datasets'\n#os.environ['HF_METRICS_CACHE'] = 'D:/huggingface/metrics'\n#os.environ['HF_MODULE_CACHE'] = 'D:/huggingface/modules'\n\nimport numpy as np\nimport pandas as pd\nimport torch\n\nfrom torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'\n\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n\n# Importing the T5 modules from huggingface/transformers\nfrom transformers import PegasusTokenizer, PegasusForConditionalGeneration\n\n# WandB – Import the wandb library\nimport wandb\nimport time\nfrom rouge_score import rouge_scorer\nimport shap\nimport sentencepiece\n\n# Project Parameters\nfilepath = r'/kaggle/input/cls-400/cls_400.csv'\n\n# predictions_filepath = '/content/gdrive/My Drive/NLP_FP/Pegasus_Finance/predictions.csv'\n\nsave_directory = '/kaggle/working/model'\n\nwandb_project_name = \"Pegasus_Summarization_Run\"\n\n!wandb login 8299a57fdc25da697310a077141526fe204ccd58\n\n!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass CustomDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, source_len, summary_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.source_len = source_len\n        self.summary_len = summary_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        source_text = str(self.data.iloc[index]['Input'])\n        summary_text = str(self.data.iloc[index]['Output'])\n\n        source_encoding = self.tokenizer.encode_plus(\n            source_text,\n            max_length=self.source_len,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt',\n            return_attention_mask=True,\n        )\n\n        summary_encoding = self.tokenizer.encode_plus(\n            summary_text,\n            max_length=self.summary_len,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt',\n            return_attention_mask=True,\n        )\n\n        return {\n            'source_ids': source_encoding['input_ids'].squeeze(),\n            'source_mask': source_encoding['attention_mask'].squeeze(),\n            'target_ids': summary_encoding['input_ids'].squeeze(),\n            'target_mask': summary_encoding['attention_mask'].squeeze(),\n        }\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(epoch, tokenizer, model, device, loader, optimizer):\n    model.train()\n    total_loss = 0.0  # Initialize the total loss for the epoch\n\n    for _, data in enumerate(loader, 0):\n        y = data['target_ids'].to(device, dtype=torch.long)\n        y_ids = y[:, :-1].contiguous()\n        labels = y[:, 1:].clone().detach()\n        labels[y[:, 1:] == tokenizer.pad_token_id] = -100  # Set padding tokens to -100\n        ids = data['source_ids'].to(device, dtype=torch.long)\n        mask = data['source_mask'].to(device, dtype=torch.long)\n\n        outputs = model(input_ids=ids, attention_mask=mask, decoder_input_ids=y_ids, labels=labels)\n        loss = outputs.loss  # Get the loss from the model's output\n\n        total_loss += loss.item()  # Accumulate the loss for this batch\n\n        if _ % 10 == 0:\n            wandb.log({\"Training Loss\": loss.item()})\n\n        if _ % 500 == 0:\n            print(f'Epoch: {epoch}, Batch: {_}, Loss: {loss.item()}')\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    average_loss = total_loss / len(loader)  # Calculate the average loss for the epoch\n    print(f'Epoch: {epoch}, Average Loss: {average_loss}')\n\n    return model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(epoch, tokenizer, model, device, loader):\n    model.eval()  # Set the model to evaluation mode\n    predictions = []\n    actuals = []\n    with torch.no_grad():\n        for _, data in enumerate(loader, 0):\n            y = data['target_ids'].to(device, dtype=torch.long)\n            ids = data['source_ids'].to(device, dtype=torch.long)\n            mask = data['source_mask'].to(device, dtype=torch.long)\n\n            # Generate summaries using the model\n            generated_ids = model.generate(\n                input_ids=ids,\n                attention_mask=mask,\n                max_length=150,\n                num_beams=2,\n                repetition_penalty=2.5,\n                length_penalty=1.0,\n                early_stopping=True\n            )\n\n            # Decode generated summaries and target summaries\n            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True) for t in y]\n\n            if _ % 100 == 0:\n                print(f'Completed {_}')\n\n            predictions.extend(preds)\n            actuals.extend(target)\n\n    return predictions, actuals\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport wandb\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom transformers import PegasusForConditionalGeneration, PegasusTokenizer\n\n\n# Define your main function\ndef main():\n    # WandB – Initialize a new run\n    wandb.init(project=wandb_project_name)\n\n    # WandB – Config is a variable that holds and saves hyperparameters and inputs\n    # Defining some key variables that will be used later on in the training  \n    config = wandb.config          # Initialize config\n    config.TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n    config.VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n    config.TRAIN_EPOCHS = 2        # number of epochs to train (default: 10)\n    config.VAL_EPOCHS = 1 \n    config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n    config.SEED = 42               # random seed (default: 42)\n    config.MAX_LEN = 512\n    config.SUMMARY_LEN = 150 \n\n    # Set random seeds and deterministic pytorch for reproducibility\n    torch.manual_seed(config.SEED) # pytorch random seed\n    np.random.seed(config.SEED) # numpy random seed\n    torch.backends.cudnn.deterministic = True\n\n    # tokenzier for encoding the text\n    #tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n    model_name = \"human-centered-summarization/financial-summarization-pegasus\"\n    tokenizer = PegasusTokenizer.from_pretrained(model_name) \n\n    # Importing and Pre-Processing the domain data\n    # Selecting the needed columns only. \n    # Adding the summarzie text in front of the text. \n    # This is to format the dataset similar to how T5 model was trained for summarization task. \n    df = pd.read_csv(filepath, encoding='latin-1')\n    df = df[['Input','Output']]\n    df.Output = 'summarize: ' + df.Output\n    print(df.head())\n\n    \n    # Creation of Dataset and Dataloader\n    # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n    train_size = 0.8\n    split = int(train_size * df.shape[0])\n    #train_dataset=df.sample(frac=train_size,random_state = config.SEED)\n    train_dataset = df.iloc[:split]\n    val_dataset = df.iloc[split:]\n    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n    train_dataset = train_dataset.reset_index(drop=True)\n\n    print(\"FULL Dataset: {}\".format(df.shape))\n    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n    print(\"TEST Dataset: {}\".format(val_dataset.shape))\n\n\n    # Creating the Training and Validation dataset for further creation of Dataloader\n    training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n    val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n\n    # Defining the parameters for creation of dataloaders\n    train_params = {\n        'batch_size': config.TRAIN_BATCH_SIZE,\n        'shuffle': True,\n        'num_workers': 0\n        }\n\n    val_params = {\n        'batch_size': config.VALID_BATCH_SIZE,\n        'shuffle': False,\n        'num_workers': 0\n        }\n\n    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n    training_loader = DataLoader(training_set, **train_params)\n    val_loader = DataLoader(val_set, **val_params)\n\n\n    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n    # Further this model is sent to device (GPU/TPU) for using the hardware.\n    #model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n    # Define your model as usual\n    model = PegasusForConditionalGeneration.from_pretrained(model_name)\n    model = model.to(device)\n\n    # Wrap the model with DataParallel to use multiple GPUs\n    if torch.cuda.device_count() > 1:\n        print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n        model = nn.DataParallel(model)\n\n\n    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n    optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n\n    # Log metrics with wandb\n    wandb.watch(model, log=\"all\")\n    # Training loop\n    print('Initiating Fine-Tuning for the model on our dataset')\n\n    for epoch in range(config.TRAIN_EPOCHS):\n    #    train(epoch, tokenizer, model, device, training_loader, optimizer)\n        fine_tuned_model = train(epoch, tokenizer, model, device, training_loader, optimizer)\n\n\n    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n    # Saving the dataframe as predictions.csv\n    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n    for epoch in range(config.VAL_EPOCHS):\n        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n        final_df.to_csv(predictions_filepath)\n        print('Output Files generated for review')\n    \n    wandb.finish()\n    return fine_tuned_model\n\nif __name__ == '__main__':\n    fine_tuned_model = main()\n    fine_tuned_model.save_pretrained(save_directory=save_directory)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}