{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install the necessary libraries here\n\n!pip install transformers -q\n!pip install wandb\n\n!pip install rouge-score\n!pip install shap\n!pip install sentencepiece\n\n# Code for TPU packages install\n# !curl -q https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-10T14:15:13.273980Z","iopub.execute_input":"2023-10-10T14:15:13.274825Z","iopub.status.idle":"2023-10-10T14:16:02.236206Z","shell.execute_reply.started":"2023-10-10T14:15:13.274790Z","shell.execute_reply":"2023-10-10T14:16:02.235045Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.15.9)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.31)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.30.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.0.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\nCollecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.23.5)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=dad51b94a29db055ee635d2f07025d43233235113c27596ae37b49008e4ded65\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\nRequirement already satisfied: shap in /opt/conda/lib/python3.10/site-packages (0.42.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from shap) (1.23.5)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from shap) (1.11.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from shap) (1.2.2)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from shap) (2.0.2)\nRequirement already satisfied: tqdm>=4.27.0 in /opt/conda/lib/python3.10/site-packages (from shap) (4.66.1)\nRequirement already satisfied: packaging>20.9 in /opt/conda/lib/python3.10/site-packages (from shap) (21.3)\nRequirement already satisfied: slicer==0.0.7 in /opt/conda/lib/python3.10/site-packages (from shap) (0.0.7)\nRequirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from shap) (0.57.1)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from shap) (2.2.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>20.9->shap) (3.0.9)\nRequirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->shap) (0.40.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->shap) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->shap) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->shap) (2023.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->shap) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->shap) (3.1.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.99)\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport os","metadata":{"execution":{"iopub.status.busy":"2023-10-10T14:16:02.238694Z","iopub.execute_input":"2023-10-10T14:16:02.239028Z","iopub.status.idle":"2023-10-10T14:16:02.244529Z","shell.execute_reply.started":"2023-10-10T14:16:02.239001Z","shell.execute_reply":"2023-10-10T14:16:02.243621Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# os.chdir(\"D:/Datasets/453_NLP_Final_Project\")\n#os.environ['TRANSFORMERS_CACHE'] = 'D:/huggingface/transformers'\n#os.environ['HF_DATASETS_CACHE'] = 'D:/huggingface/datasets'\n#os.environ['HF_METRICS_CACHE'] = 'D:/huggingface/metrics'\n#os.environ['HF_MODULE_CACHE'] = 'D:/huggingface/modules'\n\nimport numpy as np\nimport pandas as pd\nimport torch\n\nfrom torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'\n\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n\n# Importing the T5 modules from huggingface/transformers\nfrom transformers import PegasusTokenizer, PegasusForConditionalGeneration\n\n# WandB â€“ Import the wandb library\nimport wandb\nimport time\nfrom rouge_score import rouge_scorer\nimport shap\nimport sentencepiece\n\n# Project Parameters\nfilepath = r'/kaggle/input/cls-400/cls_400.csv'\n\n# predictions_filepath = '/content/gdrive/My Drive/NLP_FP/Pegasus_Finance/predictions.csv'\n\nsave_directory = '/kaggle/working/model'\n\nwandb_project_name = \"Pegasus_Summarization_Run\"\n\n!wandb login 8299a57fdc25da697310a077141526fe204ccd58\n\n!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2023-10-10T14:16:02.246058Z","iopub.execute_input":"2023-10-10T14:16:02.246996Z","iopub.status.idle":"2023-10-10T14:16:20.233790Z","shell.execute_reply.started":"2023-10-10T14:16:02.246966Z","shell.execute_reply":"2023-10-10T14:16:20.232469Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\nTue Oct 10 14:16:20 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   36C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n\nclass CustomDataset(Dataset):\n\n    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.source_len = source_len\n        self.summ_len = summ_len\n        self.text = self.data.text\n        self.ctext = self.data.ctext\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        ctext = str(self.ctext[index])\n        ctext = ' '.join(ctext.split())\n\n        text = str(self.text[index])\n        text = ' '.join(text.split())\n\n        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, \n                                                  pad_to_max_length=True,\n                                                  return_tensors='pt',\n                                                  truncation=True)\n        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, \n                                                  pad_to_max_length=True,\n                                                  return_tensors='pt',\n                                                  truncation=True)\n\n        source_ids = source['input_ids'].squeeze()\n        source_mask = source['attention_mask'].squeeze()\n        target_ids = target['input_ids'].squeeze()\n        target_mask = target['attention_mask'].squeeze()\n\n        return {\n            'source_ids': source_ids.to(dtype=torch.long), \n            'source_mask': source_mask.to(dtype=torch.long), \n            'target_ids': target_ids.to(dtype=torch.long),\n            'target_ids_y': target_ids.to(dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2023-10-10T14:16:20.239884Z","iopub.execute_input":"2023-10-10T14:16:20.242318Z","iopub.status.idle":"2023-10-10T14:16:20.259389Z","shell.execute_reply.started":"2023-10-10T14:16:20.242277Z","shell.execute_reply":"2023-10-10T14:16:20.258566Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def train(epoch, tokenizer, model, device, loader, optimizer):\n    model.train()\n    for _,data in enumerate(loader, 0):\n        y = data['target_ids'].to(device, dtype = torch.long)\n        y_ids = y[:, :-1].contiguous()\n        lm_labels = y[:, 1:].clone().detach()\n        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n        ids = data['source_ids'].to(device, dtype = torch.long)\n        mask = data['source_mask'].to(device, dtype = torch.long)\n        \n        # Changed this code from lm_labels to labels; lm_labels is deprecated - https://github.com/priya-dwivedi/Deep-Learning/issues/137\n        # outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n        loss = outputs[0]\n        \n        if _%10 == 0:\n            wandb.log({\"Training Loss\": loss.item()})\n\n        if _%500==0:\n            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        # xm.optimizer_step(optimizer)\n        # xm.mark_step()\n        return model","metadata":{"execution":{"iopub.status.busy":"2023-10-10T14:16:20.264347Z","iopub.execute_input":"2023-10-10T14:16:20.267150Z","iopub.status.idle":"2023-10-10T14:16:20.279742Z","shell.execute_reply.started":"2023-10-10T14:16:20.267117Z","shell.execute_reply":"2023-10-10T14:16:20.278820Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def validate(epoch, tokenizer, model, device, loader):\n    model.eval()\n    predictions = []\n    actuals = []\n    with torch.no_grad():\n        for _, data in enumerate(loader, 0):\n            y = data['target_ids'].to(device, dtype = torch.long)\n            ids = data['source_ids'].to(device, dtype = torch.long)\n            mask = data['source_mask'].to(device, dtype = torch.long)\n\n            generated_ids = model.generate(\n                input_ids = ids,\n                attention_mask = mask, \n                max_length=150, \n                num_beams=2,\n                repetition_penalty=2.5, \n                length_penalty=1.0, \n                early_stopping=True\n                )\n            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n            if _%100==0:\n                print(f'Completed {_}')\n\n            predictions.extend(preds)\n            actuals.extend(target)\n    return predictions, actuals","metadata":{"execution":{"iopub.status.busy":"2023-10-10T14:16:20.284424Z","iopub.execute_input":"2023-10-10T14:16:20.287342Z","iopub.status.idle":"2023-10-10T14:16:20.298900Z","shell.execute_reply.started":"2023-10-10T14:16:20.287100Z","shell.execute_reply":"2023-10-10T14:16:20.297882Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def main():\n    # WandB â€“ Initialize a new run\n    wandb.init(project=wandb_project_name)\n\n    # WandB â€“ Config is a variable that holds and saves hyperparameters and inputs\n    # Defining some key variables that will be used later on in the training  \n    config = wandb.config          # Initialize config\n    config.TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n    config.VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n    config.TRAIN_EPOCHS = 2        # number of epochs to train (default: 10)\n    config.VAL_EPOCHS = 1 \n    config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n    config.SEED = 42               # random seed (default: 42)\n    config.MAX_LEN = 512\n    config.SUMMARY_LEN = 150 \n\n    # Set random seeds and deterministic pytorch for reproducibility\n    torch.manual_seed(config.SEED) # pytorch random seed\n    np.random.seed(config.SEED) # numpy random seed\n    torch.backends.cudnn.deterministic = True\n\n    # tokenzier for encoding the text\n    #tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n    model_name = \"human-centered-summarization/financial-summarization-pegasus\"\n    tokenizer = PegasusTokenizer.from_pretrained(model_name) \n\n    # Importing and Pre-Processing the domain data\n    # Selecting the needed columns only. \n    # Adding the summarzie text in front of the text. \n    # This is to format the dataset similar to how T5 model was trained for summarization task. \n    df = pd.read_csv(filepath, encoding='latin-1')\n    df = df[['text','ctext']]\n    df.ctext = 'summarize: ' + df.ctext\n    print(df.head())\n\n    \n    # Creation of Dataset and Dataloader\n    # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n    train_size = 0.8\n    split = int(train_size * df.shape[0])\n    #train_dataset=df.sample(frac=train_size,random_state = config.SEED)\n    train_dataset = df.iloc[:split]\n    val_dataset = df.iloc[split:]\n    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n    train_dataset = train_dataset.reset_index(drop=True)\n\n    print(\"FULL Dataset: {}\".format(df.shape))\n    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n    print(\"TEST Dataset: {}\".format(val_dataset.shape))\n\n\n    # Creating the Training and Validation dataset for further creation of Dataloader\n    training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n    val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n\n    # Defining the parameters for creation of dataloaders\n    train_params = {\n        'batch_size': config.TRAIN_BATCH_SIZE,\n        'shuffle': True,\n        'num_workers': 0\n        }\n\n    val_params = {\n        'batch_size': config.VALID_BATCH_SIZE,\n        'shuffle': False,\n        'num_workers': 0\n        }\n\n    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n    training_loader = DataLoader(training_set, **train_params)\n    val_loader = DataLoader(val_set, **val_params)\n\n\n    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n    # Further this model is sent to device (GPU/TPU) for using the hardware.\n    #model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n    model = PegasusForConditionalGeneration.from_pretrained(model_name)\n    model = model.to(device)\n\n    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n    optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n\n    # Log metrics with wandb\n    wandb.watch(model, log=\"all\")\n    # Training loop\n    print('Initiating Fine-Tuning for the model on our dataset')\n\n    for epoch in range(config.TRAIN_EPOCHS):\n    #    train(epoch, tokenizer, model, device, training_loader, optimizer)\n        fine_tuned_model = train(epoch, tokenizer, model, device, training_loader, optimizer)\n\n\n    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n    # Saving the dataframe as predictions.csv\n    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n    for epoch in range(config.VAL_EPOCHS):\n        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n        final_df.to_csv(predictions_filepath)\n        print('Output Files generated for review')\n    \n    wandb.finish()\n    return fine_tuned_model\n\nif __name__ == '__main__':\n    fine_tuned_model = main()\n    fine_tuned_model.save_pretrained(save_directory=save_directory)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T14:16:20.303511Z","iopub.execute_input":"2023-10-10T14:16:20.306535Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamiteshpatra2020\u001b[0m (\u001b[33mteddyracnh\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}