{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-10-09T21:41:51.728787Z","iopub.status.busy":"2023-10-09T21:41:51.727817Z","iopub.status.idle":"2023-10-09T21:41:51.735070Z","shell.execute_reply":"2023-10-09T21:41:51.734009Z","shell.execute_reply.started":"2023-10-09T21:41:51.728748Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import glob\n","import json\n","import os\n","import sys\n","from tqdm import tqdm\n","sys.path.insert(0, '../')\n","# from utilities import *\n","from sklearn.metrics.pairwise import cosine_similarity\n","import torch"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T20:31:09.652101Z","iopub.status.busy":"2023-10-09T20:31:09.649906Z","iopub.status.idle":"2023-10-09T20:31:21.104079Z","shell.execute_reply":"2023-10-09T20:31:21.102951Z","shell.execute_reply.started":"2023-10-09T20:31:09.652066Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n"]}],"source":["# !pip install nltk"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T20:31:21.107018Z","iopub.status.busy":"2023-10-09T20:31:21.106551Z","iopub.status.idle":"2023-10-09T20:31:21.113494Z","shell.execute_reply":"2023-10-09T20:31:21.112095Z","shell.execute_reply.started":"2023-10-09T20:31:21.106979Z"},"trusted":true},"outputs":[],"source":["# import nltk\n","# nltk.download(\"all\")"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T20:31:21.115135Z","iopub.status.busy":"2023-10-09T20:31:21.114617Z","iopub.status.idle":"2023-10-09T20:31:30.995370Z","shell.execute_reply":"2023-10-09T20:31:30.994086Z","shell.execute_reply.started":"2023-10-09T20:31:21.115108Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n"]}],"source":["# !pip install transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T21:41:51.765121Z","iopub.status.busy":"2023-10-09T21:41:51.764446Z","iopub.status.idle":"2023-10-09T21:41:51.769808Z","shell.execute_reply":"2023-10-09T21:41:51.768605Z","shell.execute_reply.started":"2023-10-09T21:41:51.765089Z"},"trusted":true},"outputs":[],"source":["from nltk import tokenize\n","import nltk\n","import transformers\n","from torch.utils.data import DataLoader, TensorDataset, random_split, RandomSampler, Dataset\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T21:41:51.784083Z","iopub.status.busy":"2023-10-09T21:41:51.783368Z","iopub.status.idle":"2023-10-09T21:41:51.788872Z","shell.execute_reply":"2023-10-09T21:41:51.787983Z","shell.execute_reply.started":"2023-10-09T21:41:51.784048Z"},"trusted":true},"outputs":[],"source":["def split_to_sentences(para):\n","    sents = nltk.sent_tokenize(para)\n","    return sents\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T20:31:31.023222Z","iopub.status.busy":"2023-10-09T20:31:31.022920Z","iopub.status.idle":"2023-10-09T20:31:31.035515Z","shell.execute_reply":"2023-10-09T20:31:31.034565Z","shell.execute_reply.started":"2023-10-09T20:31:31.023183Z"},"trusted":true},"outputs":[],"source":["# import os\n","# import glob\n","# import pandas as pd\n","\n","\n","# # Get a list of all the files in the annual reports folder\n","# annual_reports_pattern = os.path.join(annual_reports_folder, \"*.txt\")\n","# annual_reports_list = glob.glob(annual_reports_pattern)\n","\n","# # Get a list of all the files in the gold summaries folder\n","# gold_summaries_pattern = os.path.join(gold_summaries_folder, \"*.txt\")\n","# gold_summaries_list = glob.glob(gold_summaries_pattern)\n","\n","# # Sort the lists to maintain order\n","# annual_reports_list.sort()\n","# gold_summaries_list.sort()\n","\n","# # Extract just the file names from the full paths\n","# annual_reports_list = [os.path.basename(file_path) for file_path in annual_reports_list]\n","# gold_summaries_list = [os.path.basename(file_path) for file_path in gold_summaries_list]\n","\n","# # Create a DataFrame with \"Input\" and \"Output\" columns\n","# data = {\n","#     \"Input\": annual_reports_list,\n","#     \"Output\": gold_summaries_list\n","# }\n","# dataset = pd.DataFrame(data)\n","\n","# # Now, 'dataset' contains two columns: \"Input\" and \"Output\" with sorted file paths\n","# dataset\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T21:41:51.807037Z","iopub.status.busy":"2023-10-09T21:41:51.806661Z","iopub.status.idle":"2023-10-09T21:41:53.161880Z","shell.execute_reply":"2023-10-09T21:41:53.161015Z","shell.execute_reply.started":"2023-10-09T21:41:51.807009Z"},"trusted":true},"outputs":[],"source":["import os\n","import glob\n","import pandas as pd\n","\n","\n","# Folder paths for annual reports and gold summaries\n","annual_reports_folder = r\"/kaggle/input/annualreports/training/training/annual_reports\"\n","gold_summaries_folder = r\"/kaggle/input/combined-data\"\n","\n","\n","# Get a list of all the files in the annual reports folder\n","annual_reports_pattern = os.path.join(annual_reports_folder, \"*.txt\")\n","annual_reports_list = glob.glob(annual_reports_pattern)\n","\n","# Get a list of all the files in the gold summaries folder\n","gold_summaries_pattern = os.path.join(gold_summaries_folder, \"*.txt\")\n","gold_summaries_list = glob.glob(gold_summaries_pattern)\n","\n","# Sort the lists to maintain order\n","annual_reports_list.sort()\n","gold_summaries_list.sort()\n","\n","# Create a DataFrame with \"Input\" and \"Output\" columns\n","data = {\n","    \"Input\": annual_reports_list,\n","    \"Output\": gold_summaries_list\n","}\n","dataset = pd.DataFrame(data)\n","\n","# Now, 'dataset' contains two columns: \"Input\" and \"Output\" with sorted file paths\n","\n","# dataset = dataset[:50]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T21:41:53.164866Z","iopub.status.busy":"2023-10-09T21:41:53.164428Z"},"trusted":true},"outputs":[],"source":["# Function to count the number of words in a text file\n","def count_words_in_file(file_path):\n","    try:\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            text = file.read()\n","            words = text.split()\n","            return len(words)\n","    except Exception as e:\n","        print(f\"Error counting words in {file_path}: {str(e)}\")\n","        return None\n","\n","# Add a new column for the length of annual reports\n","dataset['AnnualReportLength'] = dataset['Input'].apply(count_words_in_file)\n","\n","# Add a new column for the length of gold summaries\n","dataset['GoldSummaryLength'] = dataset['Output'].apply(count_words_in_file)\n","\n","# Display the updated dataset\n","# print(dataset)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T21:16:37.214702Z","iopub.status.busy":"2023-10-09T21:16:37.214365Z","iopub.status.idle":"2023-10-09T21:16:37.219289Z","shell.execute_reply":"2023-10-09T21:16:37.218032Z","shell.execute_reply.started":"2023-10-09T21:16:37.214670Z"},"trusted":true},"outputs":[],"source":["# dataset.to_excel('xyz.xlsx', index=False)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","# Assuming you already have the 'dataset' DataFrame as mentioned in the previous code\n","\n","# Initialize lists to store the contents of input and output files\n","input_contents = []\n","output_contents = []\n","\n","# Iterate through the rows of the dataset and read file contents\n","for index, row in dataset.iterrows():\n","    input_file_path = row[\"Input\"]\n","    output_file_path = row[\"Output\"]\n","    \n","    # Read the content of the input file\n","    with open(input_file_path, \"r\") as input_file:\n","        input_content = input_file.read()\n","        input_contents.append(input_content)\n","    \n","    # Read the content of the output file\n","    with open(output_file_path, \"r\") as output_file:\n","        output_content = output_file.read()\n","        output_contents.append(output_content)\n","\n","# Now, 'input_contents' and 'output_contents' contain the contents of input and output files, respectively\n","# You can access the contents using list indexing, e.g., input_contents[0], output_contents[0]\n","combined_df = pd.DataFrame({'Input_Content': input_contents, 'Output_Content': output_contents})\n","combined_df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["combined_df[\"Input_Content\"][0]"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T15:45:17.982494Z","iopub.status.busy":"2023-10-09T15:45:17.981844Z","iopub.status.idle":"2023-10-09T15:45:51.924373Z","shell.execute_reply":"2023-10-09T15:45:51.923458Z","shell.execute_reply.started":"2023-10-09T15:45:17.982463Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["DataFrame with word and character counts has been exported to /kaggle/working/dataset_with_counts.xlsx\n"]}],"source":["# import pandas as pd\n","\n","# # Function to count the number of words in a text file\n","# def count_words_in_file(file_path):\n","#     try:\n","#         with open(file_path, 'r', encoding='utf-8') as file:\n","#             text = file.read()\n","#             words = text.split()\n","#             return len(words)\n","#     except Exception as e:\n","#         print(f\"Error counting words in {file_path}: {str(e)}\")\n","#         return None\n","\n","# # Function to count the number of characters in a text file\n","# def count_characters_in_file(file_path):\n","#     try:\n","#         with open(file_path, 'r', encoding='utf-8') as file:\n","#             text = file.read()\n","#             return len(text)\n","#     except Exception as e:\n","#         print(f\"Error counting characters in {file_path}: {str(e)}\")\n","#         return None\n","\n","# # Add a new column for the length of annual reports (words and characters)\n","# dataset['AnnualReportWordCount'] = dataset['Input'].apply(count_words_in_file)\n","# dataset['AnnualReportCharacterCount'] = dataset['Input'].apply(count_characters_in_file)\n","\n","# # Add a new column for the length of gold summaries (words and characters)\n","# dataset['GoldSummaryWordCount'] = dataset['Output'].apply(count_words_in_file)\n","# dataset['GoldSummaryCharacterCount'] = dataset['Output'].apply(count_characters_in_file)\n","\n","# # Define the path for the Excel file where you want to save the DataFrame\n","# excel_file_path = \"/kaggle/working/dataset_with_counts.xlsx\"\n","\n","# # Export the updated DataFrame to an Excel file\n","# dataset.to_excel(excel_file_path, index=False)\n","\n","# # Print a message to confirm the export\n","# print(f\"DataFrame with word and character counts has been exported to {excel_file_path}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pip install --upgrade pyLDAvis\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","import os\n","import re\n","import numpy as np\n","import pandas as pd\n","import time\n","\n","import nltk\n","import pyLDAvis\n","import pyLDAvis.gensim_models\n","from nltk.corpus import stopwords\n","\n","stop_words = set(stopwords.words('english'))\n","\n","from nltk.tokenize import RegexpTokenizer\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from wordcloud import WordCloud\n","from matplotlib import pyplot as plt\n","\n","import gensim\n","import string\n","from gensim import corpora\n","from gensim.test.utils import common_texts\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","\n","import multiprocessing\n","num_cores = multiprocessing.cpu_count()\n","\n","# Check versions to ensure that there are no compatability issues:\n","python_version = !python --version\n","print(\"Python Version: \", python_version)\n","print(\"Current Directory: \", os.getcwd())\n","print(\"Numpy version: \", np.__version__)\n","print(\"Pandas version: \", pd.__version__)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T08:06:20.644683Z","iopub.status.busy":"2023-10-09T08:06:20.644268Z","iopub.status.idle":"2023-10-09T08:06:20.649365Z","shell.execute_reply":"2023-10-09T08:06:20.648540Z","shell.execute_reply.started":"2023-10-09T08:06:20.644659Z"},"trusted":true},"outputs":[],"source":["# import os\n","# import glob\n","# import pandas as pd\n","\n","# # Folder paths for annual reports and gold summaries\n","# annual_reports_folder = r\"/kaggle/input/annualreports/training/training/annual_reports\"\n","# gold_summaries_folder = r\"/kaggle/input/combined-data\"\n","\n","# # Get a list of all the files in the annual reports folder\n","# annual_reports_pattern = os.path.join(annual_reports_folder, \"*.txt\")\n","# annual_reports_list = glob.glob(annual_reports_pattern)\n","\n","# # Get a list of all the files in the gold summaries folder\n","# gold_summaries_pattern = os.path.join(gold_summaries_folder, \"*.txt\")\n","# gold_summaries_list = glob.glob(gold_summaries_pattern)\n","\n","# # Sort the lists to maintain order\n","# annual_reports_list.sort()\n","# gold_summaries_list.sort()\n","\n","# # Initialize lists to store the contents of input and output files\n","# input_contents = []\n","# output_contents = []\n","\n","# # Read the contents of the annual reports files\n","# for annual_report_file in annual_reports_list:\n","#     with open(annual_report_file, \"r\") as input_file:\n","#         input_content = input_file.read()\n","#         input_contents.append(input_content)\n","\n","# # Read the contents of the gold summaries files\n","# for gold_summary_file in gold_summaries_list:\n","#     with open(gold_summary_file, \"r\") as output_file:\n","#         output_content = output_file.read()\n","#         output_contents.append(output_content)\n","\n","# # Now, 'input_contents' and 'output_contents' contain the contents of input and output files, respectively\n","# # You can access the contents using list indexing, e.g., input_contents[0], output_contents[0]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["len(input_contents[0]), len(output_contents[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-09T21:16:37.855524Z","iopub.status.idle":"2023-10-09T21:16:37.856310Z","shell.execute_reply":"2023-10-09T21:16:37.856080Z","shell.execute_reply.started":"2023-10-09T21:16:37.856044Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T08:06:20.670454Z","iopub.status.busy":"2023-10-09T08:06:20.669891Z","iopub.status.idle":"2023-10-09T08:06:20.677779Z","shell.execute_reply":"2023-10-09T08:06:20.676775Z","shell.execute_reply.started":"2023-10-09T08:06:20.670425Z"},"trusted":true},"outputs":[],"source":["# input_contents[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def remove_non_ascii(text_list):\n","    clean_list = []\n","    for text in text_list:\n","        try:\n","            temp_encode = text.encode('ascii', 'ignore')\n","            temp_decode = temp_encode.decode('ascii')\n","        except:\n","            continue\n","        finally:\n","            clean_list.append(temp_decode)\n","    return clean_list"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# combined_df = remove_non_ascii(combined_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["unclean_full_text = list(combined_df['Input_Content'])\n","clean_full_text = remove_non_ascii(unclean_full_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["unclean_summary_text = list(combined_df['Output_Content'])\n","clean_summary_text = remove_non_ascii(unclean_summary_text)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T21:18:12.340129Z","iopub.status.busy":"2023-10-09T21:18:12.339763Z","iopub.status.idle":"2023-10-09T21:18:12.344906Z","shell.execute_reply":"2023-10-09T21:18:12.343622Z","shell.execute_reply.started":"2023-10-09T21:18:12.340102Z"},"trusted":true},"outputs":[],"source":["# clean_full_text[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["combined_df[\"Input_Content\"] = clean_full_text\n","combined_df[\"Output_Content\"] = clean_summary_text"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["combined_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# def remove_stopwords(tokenized_word_list):\n","#     clean_list = [w for w in tokenized_word_list if not w in stop_words]\n","#     return clean_list\n","\n","# def clean_text(test_string):\n","#     new_string = re.sub(r'[!\"#%&\\'()*+,-.\\/:;<=>?@\\[\\\\\\]^_`{\\|}~]|[\\d]+',\"\", test_string)\n","#     new_string2 = re.sub(r'\\w{3}.[A-Z|a-z]+.com', \"\", new_string)\n","#     new_string3 = re.sub(r'\\S+@\\S+', '', new_string2)\n","#     new_string4 = re.sub(r'http\\S+', '', new_string3)\n","#     return new_string4\n","# import re\n","\n","# def clean_text2(text):\n","#     # Define a regex pattern to remove illegal characters\n","#     illegal_char_pattern = re.compile(r'[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F\\x7F-\\x9F]')\n","    \n","#     # Use regex to remove illegal characters\n","#     cleaned_text = re.sub(illegal_char_pattern, '', text)\n","    \n","#     return cleaned_text\n","\n","\n","# # def tokenize_text(test_string):\n","# #     tokenizer = RegexpTokenizer(r'\\w+|$[0-9.]+|\\S+')\n","# #     tokenized_string = tokenizer.tokenize(test_string)\n","# #     return tokenized_string\n","\n","# # def get_LDA_model(text_data, num_topics, passes):\n","# #     start_time = time.time()\n","# #     dictionary = corpora.Dictionary(text_data)\n","# #     corpus = [dictionary.doc2bow(text) for text in text_data]\n","   \n","# #     LDA_model = gensim.models.ldamulticore.LdaMulticore(\n","# #                                corpus=corpus,\n","# #                                num_topics=num_topics,\n","# #                                id2word=dictionary,\n","# #                                workers=6,\n","# #                                passes=passes)\n","    \n","# #     topics = LDA_model.print_topics(num_words=10)\n","# #     for x in topics:\n","# #         print(x)\n","# #         print(\"\\n\")\n","# #     print(\"Time Taken: \", time.time() - start_time)\n","# #     return LDA_model, topics, corpus, dictionary"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import re\n","\n","def clean_text2(text):\n","    # Check if the input is a string\n","    if not isinstance(text, str):\n","        return text\n","    \n","    # Define a regex pattern to remove illegal characters\n","    illegal_char_pattern = re.compile(r'[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F\\x7F-\\x9F]')\n","    \n","    # Use regex to remove illegal characters\n","    cleaned_text = re.sub(illegal_char_pattern, '', text)\n","    \n","    return cleaned_text\n","combined_df['Input_Content'] = combined_df['Input_Content'].apply(clean_text2)\n","combined_df['Output_Content'] = combined_df['Output_Content'].apply(clean_text2)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T21:18:46.529285Z","iopub.status.busy":"2023-10-09T21:18:46.528771Z","iopub.status.idle":"2023-10-09T21:18:46.949729Z","shell.execute_reply":"2023-10-09T21:18:46.948418Z","shell.execute_reply.started":"2023-10-09T21:18:46.529246Z"},"trusted":true},"outputs":[],"source":["# combined_df.to_csv(\"Training_Data_Clean.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["combined_df.rename(columns={'Input_Content': 'ctext', 'Output_Content':'text'}, inplace=True)\n","combined_df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["combined_df['text'][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Remove both \"/\" and \"\\n\" characters from the 'Output_Content' column\n","combined_df['text'] = combined_df['text'].str.replace(r'[/\\n]', ' ', regex=True)\n","# Remove both \"/\" and \"\\n\" characters from the 'Output_Content' column\n","combined_df['ctext'] = combined_df['ctext'].str.replace(r'[/\\n]', ' ', regex=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["combined_df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["combined_df.to_csv(\"Training_Data_Clean.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
